# üöÄ AplicacionDeSpeedrunConAI

<img width="950" height="537" alt="Screenshot 2025-07-26 194201" src="https://github.com/user-attachments/assets/d5b357c9-9f42-4135-92ad-3ee9729bc896" />

[![Demo del proyecto](https://img.youtube.com/vi/BMqF3_rKCw8/0.jpg)](https://www.youtube.com/watch?v=BMqF3_rKCw8)


Proyecto de speedrun automatizado con IA integrada en Godot Engine usando aprendizaje por refuerzo (RL).

INTRODUCCI√ìN
PLANTEAMIENTO DEL PROBLEMA

Los speedruns en videojuegos del g√©nero plataformero requieren dominar mec√°nicas complejas como saltos precisos, sincronizaci√≥n de movimientos y adaptaci√≥n a obst√°culos din√°micos mediante estrategias optimizadas que minimicen el tiempo de completado. Sin embargo, la ejecuci√≥n manual de estas estrategias por jugadores humanos enfrenta limitaciones significativas:
Curva de aprendizaje empinada: La maestr√≠a en speedruns demanda horas de pr√°ctica para internalizar patrones y reducir errores.
Incapacidad de adaptaci√≥n en entornos din√°micos: Los obst√°culos con comportamientos no deterministas, como enemigos con trayectorias aleatorias, dificultan la optimizaci√≥n manual de rutas.
Falta de herramientas educativas integradas: Existe una brecha en recursos pedag√≥gicos que combinen el desarrollo de videojuegos con inteligencia artificial (IA) para ense√±ar aprendizaje por refuerzo (RL) de manera pr√°ctica y motivadora.
Por otro lado, los algoritmos cl√°sicos de aprendizaje por refuerzo (RL) presentan desaf√≠os cuando se aplican a entornos de videojuegos:
Espacios de acci√≥n continuos: Mec√°nicas como la intensidad variable de los saltos requieren pol√≠ticas adaptativas.
Retraso en las recompensas: Las funciones mal dise√±adas pueden incentivar soluciones miopes o ineficientes. 
Falta de generalizaci√≥n: Los agentes entrenados en niveles espec√≠ficos tienden a sobreajustarse, fallando ante cambios m√≠nimos en las condiciones del entorno.
Ante estos problemas, el presente proyecto propone desarrollar un sistema integral que combine:
Un entorno personalizado en Godot Engine, dise√±ado para desafiar y medir el progreso del agente.
Un sistema de recompensas densas y penalizaciones que gu√≠en al agente hacia estrategias √≥ptimas.
Un pipeline de entrenamiento eficiente en tiempo real que permita la adaptabilidad y el aprendizaje aut√≥nomo.

JUSTIFICACI√ìN
La inteligencia artificial, y particularmente el aprendizaje por refuerzo, ha demostrado un enorme potencial en la simulaci√≥n de comportamientos complejos dentro de entornos controlados. No obstante, la mayor√≠a de estos desarrollos se mantienen en entornos cerrados o de dif√≠cil acceso educativo.
Este proyecto busca fomentar el aprendizaje pr√°ctico de la IA aplicada a los videojuegos, dise√±ando un entorno accesible donde una IA aprenda a completar un videojuego plataformero.
Con ello se pretende:
Demostrar el potencial educativo del RL para ense√±ar conceptos de programaci√≥n y optimizaci√≥n.
Promover la innovaci√≥n en entornos interactivos que combinen entretenimiento y ciencia de datos.
Reducir la brecha digital, acercando herramientas de inteligencia artificial a comunidades estudiantiles y desarrolladores emergentes.

OBJETIVOS
Objetivo General
Desarrollar un sistema aut√≥nomo que, mediante la integraci√≥n de un entorno de videojuego en Godot Engine y un agente basado en aprendizaje por refuerzo (PPO), optimice estrategias de speedrun y demuestre capacidades de generalizaci√≥n ante entornos no vistos.
Objetivos Espec√≠ficos
Dise√±ar un entorno plataformero 2D con mec√°nicas reproducibles para el entrenamiento del agente.
Implementar una comunicaci√≥n bidireccional entre Godot y Python mediante sockets TCP/IP.
Entrenar un agente PPO con Stable Baselines3 para optimizar la velocidad de completado de los niveles.
Evaluar el rendimiento del agente frente a jugadores humanos y agentes aleatorios.
Documentar el proceso t√©cnico y metodol√≥gico para su replicaci√≥n en entornos educativos.

METODOLOGIA
La metodolog√≠a combina investigaci√≥n te√≥rica y desarrollo t√©cnico experimental, dividida en fases.
Fase te√≥rica
Revisi√≥n bibliogr√°fica de RL aplicado a videojuegos (DQN en Atari, PPO en Unity ML-Agents).
Estudio de t√©cnicas de speedrun en juegos cl√°sicos (Super Mario Bros, Celeste) para identificar mec√°nicas clave (saltos en pared, cadenas de dash).
An√°lisis de funciones de recompensa utilizadas en optimizaci√≥n temporal (reward shaping, discount factors).
Fase t√©cnica
Selecci√≥n de herramientas:
Motor de juego: Godot Engine 4.2 por su sistema modular, f√≠sicas personalizables y soporte para GDScript.
Framework de RL: Stable Baselines3 (PPO), equilibrando facilidad de integraci√≥n y rendimiento.
Protocolo de comunicaci√≥n: Socket TCP/IP, con intercambio de variables (posici√≥n, velocidad, estado del dash, datos de plataformas m√≥viles).
M√©tricas de evaluaci√≥n:
Tiempo promedio por nivel.
Eficiencia de movimientos (acciones redundantes penalizadas).
Tasa de generalizaci√≥n (% de √©xito en niveles no vistos).

## üîß Instalaci√≥n y Configuraci√≥n

### Prerrequisitos
| Herramienta | Versi√≥n m√≠nima | Enlace de descarga |
|-------------|----------------|-------------------|
| Godot Engine | 4.2 | [Descargar](https://godotengine.org/download) |
| Python | 3.10 | [Instalador](https://www.python.org/downloads/) |
| Git | 2.30+ | [Instalador](https://git-scm.com/downloads) |
| VSCode (Opcional) | 1.75+ | [Descargar](https://code.visualstudio.com/) |


### Arquitectura Implementada
Visi√≥n General
Estamos implementando una arquitectura cliente-servidor con separaci√≥n estricta de responsabilidades, dise√±ada para:

Aislar el motor del juego (Godot) de la l√≥gica de IA (Python)

Permitir entrenamiento offline de modelos RL

Facilitar la integraci√≥n continua y despliegue

Mantener alta performance en tiempo real

Capas de la Arquitectura (en orden de implementaci√≥n)
1. Capa de Presentaci√≥n (Godot Engine)
Aspecto	Detalle
Responsabilidad	Renderizado gr√°fico, interfaz de usuario y f√≠sica del juego
Tecnolog√≠as	Godot Engine 4.2+, GDScript (81.3%), C# (9.9%)
Ubicaci√≥n	Sprites/, Levels/, Scripts/Player/
Estado:	 Completado (100%)

2. Capa de Control de Juego
Aspecto	Detalle
Responsabilidad	Gesti√≥n de estados del juego, mec√°nicas y reglas
Tecnolog√≠as	GDScript, sistema de nodos de Godot
Ubicaci√≥n	Scripts/Game/
Componentes clave	game_manager.gd, level_loader.gd
Estado: Completado (100%)

3. Capa de Comunicaci√≥n
Aspecto	Detalle
Responsabilidad	Intercambio de datos entre juego y servidor de IA
Tecnolog√≠as	API REST (FastAPI), JSON over HTTP
Implementaci√≥n	Godot: Scripts/AIController/agent.gd, Python: api.py
Protocolo	HTTP POST con estado del juego ‚Üí Respuesta JSON con acci√≥n
Estado: En desarrollo (85%)

4. Capa de L√≥gica de IA
Aspecto	Detalle
Responsabilidad	Procesamiento de estados y generaci√≥n de acciones √≥ptimas
Tecnolog√≠as	Python 3.10+, Stable-Baselines3 (PPO/DQN), PyTorch
Ubicaci√≥n	api.py, stable_baselines3_example.py
Estado: En desarrollo (70%)

5. Capa de Persistencia
Aspecto	Detalle
Responsabilidad	Almacenamiento de modelos entrenados y datos de sesiones
Tecnolog√≠as	Sistema de archivos local, formato .zip para modelos
Implementaci√≥n	Directorio models/, training_logs/
Estado: Pendiente (0%)

6. Capa de Entrenamiento (Offline)
Aspecto	Detalle
Responsabilidad	Entrenamiento y optimizaci√≥n de modelos RL
Tecnolog√≠as	Python scripts, GitHub Actions (CI/CD)
Ubicaci√≥n	.github/workflows/train.yml
Estado: Pendiente (30%)
Flujo Completo de Datos

Ciclo de Vida de una Acci√≥n
Captura: Godot recolecta estado del juego (60 FPS)
Preparaci√≥n: Datos se estructuran en JSON
Transmisi√≥n: HTTP POST a localhost:5000/action

Procesamiento:
Servidor recibe estado
Modelo RL calcula mejor acci√≥n

Respuesta:
Acci√≥n serializada en JSON
Enviada de vuelta a Godot
Ejecuci√≥n: Godot aplica acci√≥n en pr√≥ximo frame
Retroalimentaci√≥n: Resultado usado para pr√≥ximo ciclo
Evoluci√≥n de la Implementaci√≥n
Fase Inicial (Completada)
Configuraci√≥n de Godot Engine
Dise√±o b√°sico de niveles
Movimiento b√°sico del personaje
Sistema de colisiones
Fase Actual (Implementando)
Integraci√≥n API REST
Comunicaci√≥n Godot-Python
Modelo RL b√°sico (PPO)
Sistema de acciones parametrizadas
Gesti√≥n de estados del juego
Pr√≥xima Fase
Entrenamiento avanzado con recompensas
Optimizaci√≥n de comunicaci√≥n
Sistema de persistencia para modelos
Integraci√≥n CI/CD con GitHub Actions
Sistema de logging y m√©tricas
Desarrollo paralelo de componentes
Actualizaciones independientes
Escalabilidad para nuevos algoritmos RL
Portabilidad entre proyectos
Monitoreo granular del rendimiento

### üîÑ Configuraci√≥n del entorno virtual de Python
```bash
# Clonar repositorio
git clone https://github.com/MathSantill/AplicacionDeSpeedrunConAI.git
cd AplicacionDeSpeedrunConAI

# Crear y activar entorno virtual (Windows)
python -m venv .venv
.venv\Scripts\activate

# Crear y activar entorno virtual (Linux/macOS)
python -m venv .venv
source .venv/bin/activate

Se recomienda estructurar los datos en formato JSON para facilidad de parsing y flexibilidad.

GitHub + GitHub Actions: para control de versiones, integraci√≥n continua y automatizaci√≥n del despliegue.
